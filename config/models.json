{
  "providers": {
    "anthropic": {
      "name": "Anthropic",
      "models": [
        {
          "value": "claude-3-5-sonnet-20241022",
          "label": "Claude 3.5 Sonnet",
          "description": "Latest Claude model with enhanced reasoning and coding capabilities",
          "context": 200000,
          "max_tokens": 8192
        },
        {
          "value": "claude-3-5-haiku-20241022",
          "label": "Claude 3.5 Haiku",
          "description": "Fast and efficient Claude model for quick tasks",
          "context": 200000,
          "max_tokens": 8192
        },
        {
          "value": "claude-3-opus-20240229",
          "label": "Claude 3 Opus",
          "description": "Most capable Claude model for complex reasoning",
          "context": 200000,
          "max_tokens": 4096
        },
        {
          "value": "claude-3-sonnet-20240229",
          "label": "Claude 3 Sonnet",
          "description": "Balanced Claude model for general tasks",
          "context": 200000,
          "max_tokens": 4096
        },
        {
          "value": "claude-3-haiku-20240307",
          "label": "Claude 3 Haiku",
          "description": "Fastest Claude model for simple tasks",
          "context": 200000,
          "max_tokens": 4096
        }
      ]
    },
    "openai": {
      "name": "OpenAI",
      "models": [
        {
          "value": "gpt-4o",
          "label": "GPT-4o",
          "description": "Latest multimodal model with vision capabilities",
          "context": 128000,
          "max_tokens": 4096
        },
        {
          "value": "gpt-4o-mini",
          "label": "GPT-4o Mini",
          "description": "Efficient and cost-effective GPT-4o variant",
          "context": 128000,
          "max_tokens": 16384
        },
        {
          "value": "gpt-4-turbo",
          "label": "GPT-4 Turbo",
          "description": "Enhanced GPT-4 with improved performance",
          "context": 128000,
          "max_tokens": 4096
        },
        {
          "value": "gpt-3.5-turbo",
          "label": "GPT-3.5 Turbo",
          "description": "Fast and cost-effective model for simple tasks",
          "context": 16385,
          "max_tokens": 4096
        },
        {
          "value": "o1-preview",
          "label": "o1 Preview",
          "description": "Experimental reasoning model with enhanced capabilities",
          "context": 128000,
          "max_tokens": 32768
        },
        {
          "value": "o1-mini",
          "label": "o1 Mini",
          "description": "Efficient reasoning model for everyday tasks",
          "context": 128000,
          "max_tokens": 65536
        }
      ]
    },
    "moonshot": {
      "name": "Moonshot AI",
      "models": [
        {
          "value": "moonshot-v1-8k",
          "label": "Moonshot v1 8k",
          "description": "Standard Moonshot model with 8k context",
          "context": 8000,
          "max_tokens": 4096
        },
        {
          "value": "moonshot-v1-32k",
          "label": "Moonshot v1 32k",
          "description": "Extended context Moonshot model",
          "context": 32000,
          "max_tokens": 4096
        },
        {
          "value": "moonshot-v1-128k",
          "label": "Moonshot v1 128k",
          "description": "Large context Moonshot model",
          "context": 128000,
          "max_tokens": 4096
        },
        {
          "value": "moonshot-chat",
          "label": "Moonshot Chat",
          "description": "Optimized for conversational tasks",
          "context": 32000,
          "max_tokens": 4096
        }
      ]
    },
    "deepseek": {
      "name": "DeepSeek",
      "models": [
        {
          "value": "deepseek-chat",
          "label": "DeepSeek Chat",
          "description": "General purpose chat model",
          "context": 32000,
          "max_tokens": 4096
        },
        {
          "value": "deepseek-coder",
          "label": "DeepSeek Coder",
          "description": "Specialized for coding tasks",
          "context": 16000,
          "max_tokens": 4096
        },
        {
          "value": "deepseek-reasoner",
          "label": "DeepSeek Reasoner",
          "description": "Enhanced reasoning capabilities",
          "context": 32000,
          "max_tokens": 4096
        }
      ]
    },
    "google": {
      "name": "Google",
      "models": [
        {
          "value": "gemini-2.0-flash",
          "label": "Gemini 2.0 Flash",
          "description": "Fast and efficient Gemini model",
          "context": 1000000,
          "max_tokens": 8192
        },
        {
          "value": "gemini-2.0-pro",
          "label": "Gemini 2.0 Pro",
          "description": "Advanced Gemini model for complex tasks",
          "context": 2000000,
          "max_tokens": 32768
        },
        {
          "value": "gemini-1.5-flash",
          "label": "Gemini 1.5 Flash",
          "description": "Lightweight Gemini model",
          "context": 1000000,
          "max_tokens": 8192
        },
        {
          "value": "gemini-1.5-pro",
          "label": "Gemini 1.5 Pro",
          "description": "Professional Gemini model",
          "context": 2000000,
          "max_tokens": 8192
        }
      ]
    },
    "meta": {
      "name": "Meta",
      "models": [
        {
          "value": "llama-3.3-70b",
          "label": "Llama 3.3 70B",
          "description": "Latest Llama model with 70B parameters",
          "context": 131072,
          "max_tokens": 4096
        },
        {
          "value": "llama-3.1-8b",
          "label": "Llama 3.1 8B",
          "description": "Efficient Llama model for everyday use",
          "context": 131072,
          "max_tokens": 4096
        },
        {
          "value": "llama-3.1-70b",
          "label": "Llama 3.1 70B",
          "description": "High-performance Llama model",
          "context": 131072,
          "max_tokens": 4096
        },
        {
          "value": "code-llama-70b",
          "label": "Code Llama 70B",
          "description": "Specialized for coding tasks",
          "context": 16384,
          "max_tokens": 4096
        }
      ]
    },
    "mistral": {
      "name": "Mistral AI",
      "models": [
        {
          "value": "mistral-large",
          "label": "Mistral Large",
          "description": "Most capable Mistral model",
          "context": 32768,
          "max_tokens": 4096
        },
        {
          "value": "mistral-medium",
          "label": "Mistral Medium",
          "description": "Balanced Mistral model",
          "context": 32768,
          "max_tokens": 4096
        },
        {
          "value": "mistral-small",
          "label": "Mistral Small",
          "description": "Efficient Mistral model",
          "context": 32768,
          "max_tokens": 4096
        },
        {
          "value": "codestral",
          "label": "Codestral",
          "description": "Specialized for coding tasks",
          "context": 32768,
          "max_tokens": 4096
        }
      ]
    }
  },
  "defaultModels": {
    "anthropic": "claude-3-5-sonnet-20241022",
    "openai": "gpt-4o",
    "moonshot": "moonshot-v1-8k",
    "deepseek": "deepseek-chat",
    "google": "gemini-2.0-flash",
    "meta": "llama-3.3-70b",
    "mistral": "mistral-large"
  }
}